import { StructuredOutputParser } from '@langchain/core/output_parsers';
import { ChatPromptTemplate } from '@langchain/core/prompts';
import { RunnableLambda, RunnableSequence } from '@langchain/core/runnables';
import { ChatDeepSeek } from '@langchain/deepseek';
import type { ChatOpenAI } from '@langchain/openai';
import { Injectable } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import {
	lookupResultSchema,
	ProjectMinedDto,
	projectMinedSchema,
	ProjectPolishedDto,
	projectPolishedSchema,
	projectSchema
} from '@prism-ai/shared';
import { BufferMemory } from 'langchain/memory';
import * as path from 'path';
import { z } from 'zod';
import { AgentService } from '../agent/agent.service';
import { MCPClientService } from '../mcp-client/mcp-client.service';
import { ModelService } from '../model/model.service';
import { PromptService, role } from '../prompt/prompt.service';

@Injectable()
export class ChainService {
	constructor(
		public modelService: ModelService,
		public promptService: PromptService,
		private agentService: AgentService,
		public clientService: MCPClientService,
		public configService: ConfigService
	) {}

	/**
	 * åˆ›å»ºé“¾, memoryé»˜è®¤ä½¿ç”¨BufferMemory, memoryæ˜¯å¦æ³¨å…¥promptå–å†³äºpromptæ˜¯å¦æä¾›{chat_history}æ’æ§½
	 * @description chat_record -> memory -> chat_history -> prompt -> llm
	 * @param llm æ¨¡å‹å®ä¾‹
	 * @param prompt è¾“å…¥æ¨¡å‹çš„æ•´ä¸ªprompt
	 * @param schema å®šä¹‰æ¨¡å‹è¾“å‡ºæ ¼å¼çš„zod schema
	 * @param saveFn ç»“æœä¿å­˜å‡½æ•°,ä¿å­˜åˆ°mongodbæ•°æ®åº“
	 */
	private async createChain<Input = string, Output = unknown>(
		llm: ChatOpenAI | ChatDeepSeek,
		prompt: ChatPromptTemplate,
		outputSchema: z.Schema,
		inputSchema?: z.Schema
	): Promise<RunnableSequence<Input, Output>> {
		const outputParser = StructuredOutputParser.fromZodSchema(outputSchema);

		const memory = new BufferMemory({
			chatHistory: this.modelService.getChatHistory(
				`${new Date().toLocaleDateString().replace(/\//g, '-')}`
			)
		});

		let userInput = '';
		const chain = RunnableSequence.from<Input, Output>([
			{
				input: input => {
					userInput = input as string;
					return input;
				},
				chat_history: async (input: any, options: any) => {
					const vars = await memory.loadMemoryVariables({ input }); //EntityMemoryéœ€è¦ä¼ å…¥input
					return vars.history || vars.summary || '';
				},
				instructions: async () => {
					return outputParser.getFormatInstructions();
				},
				/* å½“è¾“å‡ºåŒ…å«è¾“å…¥æ ¼å¼çš„è¾“å‡ºæ•°æ®æ—¶,éœ€è¦å‘æ¨¡å‹æŒ‡å®š */
				instructions0: async () => {
					const outputParser = inputSchema && StructuredOutputParser.fromZodSchema(inputSchema);
					return outputParser && outputParser.getFormatInstructions();
				}
			},
			prompt,
			llm,
			outputParser,
			RunnableLambda.from(async input => {
				await memory.saveContext({ input: userInput }, { output: input });
				return input;
			})
		]);

		return chain;
	}

	/**
	 * åˆ›å»ºæµå¼é“¾ï¼ˆä¸åŒ…å«ä¿å­˜é€»è¾‘ï¼‰
	 */
	private async createStreamChain<Input = string>(
		llm: ChatOpenAI | ChatDeepSeek,
		prompt: ChatPromptTemplate,
		outputSchema: z.Schema,
		inputSchema?: z.Schema
	): Promise<RunnableSequence<Input, any>> {
		const memory = new BufferMemory({
			chatHistory: this.modelService.getChatHistory(
				`${new Date().toLocaleDateString().replace(/\//g, '-')}`
			)
		});

		const chain = RunnableSequence.from<Input, any>([
			{
				input: input => input,
				chat_history: async (input: any) => {
					const vars = await memory.loadMemoryVariables({ input });
					return vars.history || vars.summary || '';
				},
				instructions: async () => {
					const outputParser = StructuredOutputParser.fromZodSchema(outputSchema);
					const a = outputParser.getFormatInstructions();
					console.log('ğŸš€ ~ instructions: ~ a:', a);
					return a;
				},
				/* å½“è¾“å‡ºåŒ…å«è¾“å…¥æ ¼å¼çš„è¾“å‡ºæ•°æ®æ—¶,éœ€è¦å‘æ¨¡å‹æŒ‡å®š */
				instructions0: async () => {
					const outputParser = inputSchema && StructuredOutputParser.fromZodSchema(inputSchema);
					return outputParser && outputParser.getFormatInstructions();
				}
			},
			prompt,
			llm
			// ä¸æ·»åŠ ä¼šé˜»æˆªæµå¼è¾“å‡ºçš„Runnable
		]);

		return chain;
	}

	/**
	 * è¾“å…¥çš„æ–‡æœ¬é¡¹ç›®ç»éªŒï¼ˆå•ä¸ªï¼‰è½¬åŒ–ä¸ºJSON
	 * @description 1ã€ç”¨æˆ·å¯¼å…¥ç°æœ‰çš„é¡¹ç›®ç»éªŒ,åˆ™é€šè¿‡llmè½¬ä¸ºJSON
	 * @description 2ã€ç”¨æˆ·ä»¥è¡¨å•æäº¤é¡¹ç›®ç»éªŒ,åˆ™ç›´æ¥å°±æ˜¯JSON
	 */
	async tansformChain() {
		const outputParser = StructuredOutputParser.fromZodSchema(projectSchema);
		const prompt = ChatPromptTemplate.fromMessages([
			[
				`${role.SYSTEM}`,
				`
				å°†ç”¨æˆ·è¾“å…¥çš„é¡¹ç›®ç»éªŒæè¿°æŒ‰æŒ‡å®šæ ¼å¼è¾“å‡ºã€‚
				å¦‚æœä¿¡æ¯ç¼ºå¤±,å°±ç•™ç©ºã€‚
				æ³¨æ„ä¸è¦ä¿®æ”¹ä»»ä½•ä¿¡æ¯ã€‚
				ä½ éœ€è¦å¯¹äº®ç‚¹è¿›è¡Œåˆ†ç±»,ä½†ä¸è¦ä¿®æ”¹äº®ç‚¹çš„ä»»ä½•ä¿¡æ¯ã€‚
				æ ¼å¼è¯´æ˜:{instructions}` // å†…éƒ¨çš„promptä¼šæ•™JSON schemaã€ç»™è¾“å…¥çš„JSON schemaç»™llm
			],
			[`${role.HUMAN}`, '{input}']
		]);

		const llm = await this.modelService.getLLMDeepSeekRaw('deepseek-chat');
		const chain = RunnableSequence.from<{ input: string }, z.infer<typeof projectSchema>>([
			{
				input: input => input.input,
				instructions: () => outputParser.getFormatInstructions()
			},
			prompt,
			llm,
			outputParser
		]);
		return chain;
	}

	/**
	 * æ ¼å¼ä¿®å¤ï¼šæŒ‰schemaæŒ‡å®šçš„æ ¼å¼å°†åŸè¾“å…¥è¾“å‡º
	 * @param schema zod schema
	 * @param input åŸè¾“å…¥
	 * @param errMsg æ ¼å¼é”™è¯¯ä¿¡æ¯
	 * @returns
	 */
	async fomartFixChain<T = any>(schema: z.Schema, errMsg: string) {
		const llm = this.modelService.getLLMDeepSeekRaw('deepseek-chat');
		const outputParser = StructuredOutputParser.fromZodSchema(schema);
		const prompt = ChatPromptTemplate.fromMessages([
			[
				`${role.SYSTEM}`,
				`
				ç”¨æˆ·å°†è¾“å…¥æ ¼å¼é”™è¯¯çš„é¡¹ç›®ç»éªŒæè¿°ã€‚
				æ ¹æ®ä»¥ä¸‹æ ¼å¼è¯´æ˜å’Œé”™è¯¯ä¿¡æ¯ä¿®å¤æ ¼å¼é”™è¯¯ã€‚
				æ³¨æ„ä¸è¦ä¿®æ”¹ä»»ä½•ä¿¡æ¯ã€‚
				æ ¼å¼è¯´æ˜:{instructions}
				é”™è¯¯ä¿¡æ¯:{errMsg}
				`
			],
			[`${role.HUMAN}`, '{input}']
		]);
		const chain = RunnableSequence.from<{ input: string }, T>([
			{
				input: input => input.input,
				instructions: () => outputParser.getFormatInstructions(),
				errMsg: () => errMsg
			},
			prompt,
			llm,
			outputParser
		]);
		return chain;
	}

	/**
	 * ç°æœ‰äº®ç‚¹è¯„ä¼°ã€æ”¹è¿›ã€‚
	 * @description -> äº®ç‚¹çªå‡º
	 */
	async polishChain(stream = false) {
		const schema = projectPolishedSchema;
		const schema0 = projectSchema; // è¾“å…¥çš„schema
		const outputParser = StructuredOutputParser.fromZodSchema(schema);
		const prompt = await this.promptService.polishPrompt();

		const llm = await this.modelService.getLLMDeepSeekRaw('deepseek-reasoner');

		const chain = await this.createChain<string, ProjectPolishedDto>(llm, prompt, schema, schema0);
		const streamChain = await this.createStreamChain<string>(llm, prompt, schema, schema0);
		if (stream) {
			return streamChain;
		}
		return chain;
	}

	/**
	 * é¡¹ç›®äº®ç‚¹æŒ–æ˜ã€‚
	 * @description -> äº®ç‚¹å……è¶³
	 */
	async mineChain(stream = false) {
		const schema = projectMinedSchema;
		const schema0 = projectSchema; // è¾“å…¥çš„schema

		const outputParser = StructuredOutputParser.fromZodSchema(schema);
		const prompt = await this.promptService.minePrompt();

		const llm = await this.modelService.getLLMDeepSeekRaw('deepseek-reasoner');

		const chain = await this.createChain<string, ProjectMinedDto>(llm, prompt, schema, schema0);
		const streamChain = await this.createStreamChain<string>(llm, prompt, schema, schema0);
		if (stream) {
			return streamChain;
		}
		return chain;
	}

	/**
	 * åˆ†æé¡¹ç›®ç»éªŒçš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
	 */
	async lookupChain(stream = false) {
		const schema = lookupResultSchema;
		const prompt = await this.promptService.lookupPrompt();

		const llm = await this.modelService.getLLMDeepSeekRaw('deepseek-reasoner');

		const chain = await this.createChain<string, ProjectMinedDto>(llm, prompt, schema);
		const streamChain = await this.createStreamChain<string>(llm, prompt, schema);
		if (stream) {
			return streamChain;
		}
		return chain;
	}

	/**
	 * é€šè¿‡agentå’ŒmcpæŸ¥è¯¢æœ¬åœ°mongodbæ•°æ®åº“
	 * @param query ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢è¯­å¥
	 */
	async queryChain() {
		try {
			const llm = await this.modelService.getLLMDeepSeekRaw('deepseek-reasoner');
			// const llm = await this.modelService.getLLMOpenAIRaw();

			const client = await this.clientService.connectToServerLocal(
				'mongodb',
				path.join(process.cwd(), './mcp-servers.json')
			);

			const tools = await this.clientService.getTools(client);

			// æ·»åŠ é¡¹ç›®è¡¨ç»“æ„ä¿¡æ¯åˆ°ç³»ç»Ÿæç¤ºä¸­
			const prompt = ChatPromptTemplate.fromMessages([
				[
					`${role.SYSTEM}`,
					`ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢é¡¹ç›®æ•°æ®åº“ã€‚
æ•°æ®åº“ä¸­çš„projectsé›†åˆå­—æ®µä¸¾ä¾‹è¯´æ˜å¦‚ä¸‹ï¼š
{{
  "info": {{
    "name": "Ul ç»„ä»¶åº“",
    "desc": {{
      "role": "è´Ÿè´£ç»„ä»¶æ¶æ„è®¾è®¡ã€æ ¸å¿ƒåŠŸèƒ½å¼€å‘åŠè´¨é‡ä¿éšœå·¥ä½œï¼Œä¸»å¯¼æŠ€æœ¯é€‰å‹ä¸å·¥ç¨‹åŒ–å»ºè®¾",
      "contribute": "ç‹¬ç«‹å¼€å‘20+ä¸ªåŸºç¡€ç»„ä»¶ï¼Œå®ç°Monorepoå¤šåŒ…ç®¡ç†æ¶æ„ï¼Œå»ºç«‹å®Œæ•´çš„ä»£ç è§„èŒƒä½“ç³»ä¸è‡ªåŠ¨åŒ–æµ‹è¯•æ–¹æ¡ˆ",
      "bgAndTarget": "æ„å»ºä¼ä¸šçº§UIç»„ä»¶åº“ä»¥ç»Ÿä¸€äº§å“è®¾è®¡è¯­è¨€ï¼Œæä¾›å¯å¤ç”¨çš„å‰ç«¯ç»„ä»¶èµ„äº§ï¼Œæå‡è·¨å›¢é˜Ÿåä½œæ•ˆç‡",
      "_id": {{
        "$oid": "681b16119199e6ef8f1952d1"
				}}
    }},
    "techStack": [
      "React",
      "Sass",
      "Axios",
      "TypeScript",
      "StoryBook",
      "Testing Library"
    ],
    "_id": {{
      "$oid": "681b16119199e6ef8f1952d0"
    }}
}}
ä½¿ç”¨æä¾›çš„å·¥å…·æ¥æŸ¥è¯¢æ•°æ®åº“ã€‚`
				], //ä¼˜åŒ–ï¼šåœ¨system prompté‡Œå°†è¡¨ç»“æ„ä¿¡æ¯ï¼Œå’Œæ›´æ˜ç¡®çš„è¦æ±‚å‘Šè¯‰æ¨¡å‹ï¼ˆå›ºå®šä»»åŠ¡ä¸åº”è¯¥è®©llmè‡ªå·±æ¨ç†å¤ªå¤šï¼‰
				[`${role.PLACEHOLDER}`, `{chat_history}`],
				[`${role.HUMAN}`, '{input}'],
				[`${role.PLACEHOLDER}`, `{agent_scratchpad}`]
			]);

			const agent = await this.agentService.createOpenAIToolsAgent(llm, client, tools, prompt);

			const memory = new BufferMemory({
				chatHistory: this.modelService.getChatHistory(
					`${new Date().toLocaleDateString().replace(/\//g, '-')}`
				)
			});

			let userInput = '';
			const chain = RunnableSequence.from<string, string>([
				RunnableLambda.from(async input => {
					userInput = input;
					const vars = await memory.loadMemoryVariables({ input }); //EntityMemoryéœ€è¦ä¼ å…¥input
					return {
						input,
						chat_history: vars.history || vars.summary || ''
					};
				}),
				agent, //! promptå·²ç»åœ¨agenté‡Œç®¡ç†äº†,ä¸è¦å†åœ¨chainé‡ŒåŠ prompt ï¼ˆå’Œå•çº¯llmä¸åŒï¼‰
				//! StringOutputParser ä¼šå‡ºé”™ å…¶_baseMessageContentToStringæ‹¿ä¸åˆ°å€¼
				RunnableLambda.from((input: any) => {
					//æ”¹ç”¨è‡ªå®šä¹‰çš„StringOutputParser
					if (
						typeof input === 'object' &&
						('output' in input || 'text' in input || 'content' in input)
					) {
						return String(input.output ?? input.text ?? input.content);
					} else if (Array.isArray(input)) {
						return input
							.map(item => {
								if (
									typeof item === 'object' &&
									('output' in item || 'text' in item || 'content' in item)
								) {
									return String(item.output ?? item.text ?? item.content);
								}
								return String(item);
							})
							.join('\n');
					}
					return String(input.content);
				}),
				RunnableLambda.from(async input => {
					await memory.saveContext({ input: userInput }, { output: input });
					return input;
				})
			]);
			return chain;
		} catch (error) {
			console.error('åˆ›å»ºæŸ¥è¯¢é“¾å¤±è´¥:', error);
			throw error;
		}
	}
}
